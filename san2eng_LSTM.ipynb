{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fn-h7-kgDcum",
        "outputId": "07630475-9f1a-4849-86f7-891f161ec84e"
      },
      "outputs": [],
      "source": [
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q__QONxy5Qbx"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "from numpy import array, argmax, random, take\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from array import array\n",
        "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "%matplotlib inline\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "import collections\n",
        "import helper\n",
        "import pathlib\n",
        "import glob\n",
        "import os\n",
        "\n",
        "\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMgUkRQW5XGS",
        "outputId": "6731a1c4-1af6-4e25-a948-86b2cbf4934f"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mObWgco5dRO"
      },
      "outputs": [],
      "source": [
        "# function to read raw text file\n",
        "def read_text(filename):\n",
        "        # open the file\n",
        "        file = open(filename, mode='rt', encoding='utf-8')\n",
        "\n",
        "        # read all text\n",
        "        text = file.read()\n",
        "        file.close()\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lt7h5VBo6DJf",
        "outputId": "30681a06-52a8-4d47-a464-378671462527"
      },
      "outputs": [],
      "source": [
        "#!ls \"/content/gdrive/MyDrive/ramayana_1-6.txt\"\n",
        "!ls \"/content/gdrive/MyDrive/buddhistSkrt.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyzm0J8w6Lr5"
      },
      "outputs": [],
      "source": [
        "#data = read_text(\"/content/gdrive/MyDrive/ramayana_1-6.txt\")\n",
        "data = read_text(\"/content/gdrive/MyDrive/buddhistSkrt.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "7hz4yWB3Q1_T",
        "outputId": "4904a393-fc81-462c-f1e5-5cfb1ccda5b9"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0479EdF56raW"
      },
      "outputs": [],
      "source": [
        "# split a text into sentences\n",
        "def to_lines(text):\n",
        "      sents = text.strip().split('\\n')\n",
        "      sents = [i.split(',') for i in sents]\n",
        "      return sents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "244LWf066UHv",
        "outputId": "ba5e7402-b9fa-49f7-a7e2-1962bc429365"
      },
      "outputs": [],
      "source": [
        "eng_san = to_lines(data)\n",
        "eng_san = np.array(eng_san)\n",
        "eng_san = eng_san[:]\n",
        "eng_san"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T681vOyxfqwM",
        "outputId": "44c964f2-ef16-4e58-bf95-e79f5c5962e7"
      },
      "outputs": [],
      "source": [
        "\n",
        "len(eng_san)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbBou3gPESoV",
        "outputId": "9852355a-a8cd-401b-e1bc-c36bced6285f"
      },
      "outputs": [],
      "source": [
        "print(eng_san)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "oyD2mMYm6nSf",
        "outputId": "491b361e-1c76-4122-9467-5d27936b8818"
      },
      "outputs": [],
      "source": [
        "# Remove Punctuation\n",
        "eng_san[1] = [s.translate(str.maketrans('', '',  string.punctuation)) for s in eng_san[1]]\n",
        "eng_san[4] = [s.translate(str.maketrans('', '', string.punctuation)) for s in eng_san[4]]\n",
        "print(eng_san[0][1])\n",
        "eng_san[0][4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "uJFGm9vZ68qX",
        "outputId": "9756c5fa-2d17-4664-b952-85c69f3d25e3"
      },
      "outputs": [],
      "source": [
        "# Converting text to lower case\n",
        "for i in range(len(eng_san)):\n",
        "    eng_san[1] = eng_san[1].lower()\n",
        "    eng_san[4] = eng_san[4].lower()\n",
        "\n",
        "eng_san[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "nY23xGoF6-3K",
        "outputId": "4b368cca-4953-43ba-9bb6-d93cac260fcf"
      },
      "outputs": [],
      "source": [
        "# Converting Text to Sequence\n",
        "# empty lists\n",
        "eng_l = []\n",
        "san_l = []\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in eng_san[1]:\n",
        "      eng_l.append(len(i.split()))\n",
        "\n",
        "for i in eng_san[4]:\n",
        "      san_l.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'eng':eng_l, 'san':san_l})\n",
        "\n",
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAGYC9S07EHD"
      },
      "outputs": [],
      "source": [
        "# Build Tokenizer\n",
        "def tokenization(lines):\n",
        "      tokenizer = Tokenizer()\n",
        "      tokenizer.fit_on_texts(lines)\n",
        "      return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLHmZ8Ob7G3F",
        "outputId": "012abc48-070d-43d5-f160-95cabe55dc1d"
      },
      "outputs": [],
      "source": [
        "# Prepare English Tokenizer\n",
        "eng_tokenizer = tokenization(eng_san[1])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "\n",
        "eng_length = 8\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTggCDF_7ImA",
        "outputId": "b41c1294-411e-4613-9db3-c3a825cb70b9"
      },
      "outputs": [],
      "source": [
        "# Prepare Spanish Tokenizer\n",
        "san_tokenizer = tokenization(eng_san[4])\n",
        "san_vocab_size = len(san_tokenizer.word_index) + 1\n",
        "\n",
        "san_length = 8\n",
        "print('Sanskrit Vocabulary Size: %d' % san_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqhgfQTD7KmF"
      },
      "outputs": [],
      "source": [
        "# Encode and Pad Sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "         # integer encode sequences\n",
        "         seq = tokenizer.texts_to_sequences(lines)\n",
        "         # pad sequences with 0 values\n",
        "         seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "         return seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9QN4Ie17M0C"
      },
      "outputs": [],
      "source": [
        "# split data into train and test set\n",
        "train, test = train_test_split(eng_san, test_size=0.2, random_state = 12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UzM84AB7OZZ"
      },
      "outputs": [],
      "source": [
        "# prepare training data\n",
        "trainX = encode_sequences(san_tokenizer, san_length, train[4])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[1])\n",
        "\n",
        "# prepare validation data\n",
        "testX = encode_sequences(san_tokenizer, san_length, test[4])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlefvdYN7QKO"
      },
      "outputs": [],
      "source": [
        "# build NMT model\n",
        "def define_model(in_vocab,out_vocab, in_timesteps,out_timesteps,units):\n",
        "      model = Sequential()\n",
        "      model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
        "      model.add(LSTM(units))\n",
        "      model.add(RepeatVector(out_timesteps))\n",
        "      model.add(LSTM(units, return_sequences=True))\n",
        "      model.add(Dense(out_vocab, activation='softmax'))\n",
        "      return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQOL01oN7SKE"
      },
      "outputs": [],
      "source": [
        "# model compilation\n",
        "model = define_model(san_vocab_size, eng_vocab_size, san_length, eng_length, 512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efVQVVZ_7Tzb"
      },
      "outputs": [],
      "source": [
        "rms = optimizers.RMSprop(learning_rate=0.001)\n",
        "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L1Uuf8p7Vku",
        "outputId": "e536f7ea-1035-4b0b-dd79-a348ea7f8a36"
      },
      "outputs": [],
      "source": [
        "# train model\n",
        "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n",
        "                    epochs=75, batch_size=512, validation_split = 0.2, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "JQpdCQOW7XSo",
        "outputId": "e665bf77-5308-4438-e779-0ef9566ab06b"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['train','validation'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P--bKa3i7aCV",
        "outputId": "e2fdc116-539e-4dc8-93b5-64691366af41"
      },
      "outputs": [],
      "source": [
        "preds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6iGmAeu7bvu"
      },
      "outputs": [],
      "source": [
        "def get_word(n, tokenizer):\n",
        "      for word, index in tokenizer.word_index.items():\n",
        "          if index == n:\n",
        "              return word\n",
        "      return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oll5bmzE7eMq"
      },
      "outputs": [],
      "source": [
        "preds_text = []\n",
        "for i in preds:\n",
        "       temp = []\n",
        "       for j in range(len(i)):\n",
        "            t = get_word(i[j], eng_tokenizer)\n",
        "            if j > 0:\n",
        "                if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
        "                     temp.append('')\n",
        "                else:\n",
        "                     temp.append(t)\n",
        "            else:\n",
        "                   if(t == None):\n",
        "                          temp.append('')\n",
        "                   else:\n",
        "                          temp.append(t)\n",
        "\n",
        "       preds_text.append(' '.join(temp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeOR0apM7gOA"
      },
      "outputs": [],
      "source": [
        "pred_df = pd.DataFrame({'actual' : test[1], 'predicted' : preds_text})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "id": "QQp2ip0z7jPE",
        "outputId": "c1745a28-5b72-4815-d92e-1c7faf02f58a"
      },
      "outputs": [],
      "source": [
        "pred_df.sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fw3ejQypLVq7"
      },
      "outputs": [],
      "source": [
        "def translate(sent):\n",
        "  user_sents = []\n",
        "  user_sents.append(sent)\n",
        "  userTest = encode_sequences(san_tokenizer, san_length, user_sents)\n",
        "  pred = model.predict_classes(userTest)\n",
        "  user_text = []\n",
        "  for i in pred:\n",
        "        temp = []\n",
        "        for j in range(len(i)):\n",
        "              t = get_word(i[j], eng_tokenizer)\n",
        "              if j > 0:\n",
        "                  if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
        "                      temp.append('')\n",
        "                  else:\n",
        "                      temp.append(t)\n",
        "              else:\n",
        "                    if(t == None):\n",
        "                            temp.append('')\n",
        "                    else:\n",
        "                            temp.append(t)\n",
        "\n",
        "        user_text.append(' '.join(temp))\n",
        "  return user_text[0]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
